{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahirunie-dulsara/EN3150-Assignment-3-CNN/blob/Sakith/RealWaste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHbFqHHu0YqR",
        "outputId": "2250b717-a340-4c43-8a8a-bc642fe397a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FQcAlzHU2Sw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c55b74-aee4-4d52-88ca-34f862f29938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Unzipped to: /content/extracted_zip\n"
          ]
        }
      ],
      "source": [
        "import zipfile, os, re, shutil\n",
        "from PIL import Image\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/Real Waste.zip\"\n",
        "\n",
        "extract_path = \"/content/extracted_zip\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"‚úÖ Unzipped to:\", extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/extracted_zip/realwaste-main/RealWaste\"\n",
        "for folder in os.listdir(base_dir):\n",
        "    count = len(os.listdir(os.path.join(base_dir, folder)))\n",
        "    print(f\"{folder}: {count} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC34Jsja_TlT",
        "outputId": "0c804d38-b8d6-4843-bf04-d94e69fbb7c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textile Trash: 318 images\n",
            "Paper: 500 images\n",
            "Food Organics: 411 images\n",
            "Glass: 420 images\n",
            "Miscellaneous Trash: 495 images\n",
            "Vegetation: 436 images\n",
            "Plastic: 921 images\n",
            "Cardboard: 461 images\n",
            "Metal: 790 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install split-folders"
      ],
      "metadata": {
        "id": "QfEBmPX8JGfM",
        "outputId": "93046597-d6e8-400d-9e42-4896933476bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders  # install with: pip install split-folders\n",
        "\n",
        "input_folder = \"/content/extracted_zip/realwaste-main/RealWaste\"\n",
        "output_folder = \"/content/extracted_zip/realwaste-main/realwaste_split\"\n",
        "\n",
        "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.7, .15, .15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op1hnovlItS8",
        "outputId": "244f9f28-931d-4ba1-9b54-da4b104ed754"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 4752 files [00:04, 1151.28 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/extracted_zip/realwaste-main/realwaste_split\"\n",
        "\n",
        "# Count images in each subfolder\n",
        "for split in ['train', 'val', 'test']:\n",
        "    split_path = os.path.join(base_dir, split)\n",
        "    print(f\"\\nüìÅ {split.upper()} SET\")\n",
        "    total = 0\n",
        "    for cls in os.listdir(split_path):\n",
        "        cls_path = os.path.join(split_path, cls)\n",
        "        count = len(os.listdir(cls_path))\n",
        "\n",
        "        total += count\n",
        "        print(f\"  {cls}: {count} images\")\n",
        "    print(f\"  ‚ûú Total {split}: {total} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb9khb9XKCQK",
        "outputId": "6bc8453c-cb04-44fe-e7a6-a189eda9b347"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÅ TRAIN SET\n",
            "  Textile Trash: 222 images\n",
            "  Paper: 350 images\n",
            "  Food Organics: 287 images\n",
            "  Glass: 294 images\n",
            "  Miscellaneous Trash: 346 images\n",
            "  Vegetation: 305 images\n",
            "  Plastic: 644 images\n",
            "  Cardboard: 322 images\n",
            "  Metal: 553 images\n",
            "  ‚ûú Total train: 3323 images\n",
            "\n",
            "üìÅ VAL SET\n",
            "  Textile Trash: 47 images\n",
            "  Paper: 75 images\n",
            "  Food Organics: 61 images\n",
            "  Glass: 63 images\n",
            "  Miscellaneous Trash: 74 images\n",
            "  Vegetation: 65 images\n",
            "  Plastic: 138 images\n",
            "  Cardboard: 69 images\n",
            "  Metal: 118 images\n",
            "  ‚ûú Total val: 710 images\n",
            "\n",
            "üìÅ TEST SET\n",
            "  Textile Trash: 49 images\n",
            "  Paper: 75 images\n",
            "  Food Organics: 63 images\n",
            "  Glass: 63 images\n",
            "  Miscellaneous Trash: 75 images\n",
            "  Vegetation: 66 images\n",
            "  Plastic: 139 images\n",
            "  Cardboard: 70 images\n",
            "  Metal: 119 images\n",
            "  ‚ûú Total test: 719 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\"/content/extracted_zip/realwaste-main/realwaste_split/train\", transform=train_transforms)\n",
        "val_dataset   = datasets.ImageFolder(\"/content/extracted_zip/realwaste-main/realwaste_split/val\", transform=val_test_transforms)\n",
        "test_dataset  = datasets.ImageFolder(\"/content/extracted_zip/realwaste-main/realwaste_split/test\", transform=val_test_transforms)\n"
      ],
      "metadata": {
        "id": "mfh7AZ8-15hW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class_counts = np.bincount(train_dataset.targets)\n",
        "print(\"Class counts:\", class_counts)\n",
        "\n",
        "# Compute class weights (inverse of frequency)\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "\n",
        "sample_weights = [class_weights[label] for label in train_dataset.targets]\n",
        "\n",
        "print(\"üìä Class counts per category:\")\n",
        "for cls, count in zip(train_dataset.classes, class_counts):\n",
        "    print(f\"  {cls:15s}: {count}\")\n",
        "\n",
        "print(\"\\n‚öñÔ∏è Class weights (inverse of frequency):\")\n",
        "for cls, w in zip(train_dataset.classes, class_weights):\n",
        "    print(f\"  {cls:15s}: {w:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmQbScjmCmkv",
        "outputId": "501ead8f-6a8d-4d59-aa6b-73fb3f584099"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class counts: [322 287 294 553 346 350 644 222 305]\n",
            "üìä Class counts per category:\n",
            "  Cardboard      : 322\n",
            "  Food Organics  : 287\n",
            "  Glass          : 294\n",
            "  Metal          : 553\n",
            "  Miscellaneous Trash: 346\n",
            "  Paper          : 350\n",
            "  Plastic        : 644\n",
            "  Textile Trash  : 222\n",
            "  Vegetation     : 305\n",
            "\n",
            "‚öñÔ∏è Class weights (inverse of frequency):\n",
            "  Cardboard      : 0.003106\n",
            "  Food Organics  : 0.003484\n",
            "  Glass          : 0.003401\n",
            "  Metal          : 0.001808\n",
            "  Miscellaneous Trash: 0.002890\n",
            "  Paper          : 0.002857\n",
            "  Plastic        : 0.001553\n",
            "  Textile Trash  : 0.004505\n",
            "  Vegetation     : 0.003279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,      # weight per sample\n",
        "    num_samples=len(sample_weights),  # total samples to draw per epoch\n",
        "    replacement=True             # allow repeated samples\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
        "\n",
        "print(f\"\\n‚úÖ WeightedRandomSampler created successfully!\")\n",
        "print(f\"‚û°Ô∏è Total samples in epoch: {len(sample_weights)}\")\n",
        "print(f\"‚û°Ô∏è Batch size: {train_loader.batch_size}\")\n",
        "print(f\"‚û°Ô∏è Total batches per epoch: {len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHJPQoRzMefy",
        "outputId": "cb722720-3703-42c6-d009-f865c21e5a08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ WeightedRandomSampler created successfully!\n",
            "‚û°Ô∏è Total samples in epoch: 3323\n",
            "‚û°Ô∏è Batch size: 32\n",
            "‚û°Ô∏è Total batches per epoch: 104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class WasteCNN(nn.Module):\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(WasteCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 32x112x112\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 64x56x56\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 128x28x28\n",
        "        x = x.view(-1, 128 * 28 * 28)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Vip9TVk2VxQy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"‚úÖ CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "id": "tvZJqBeRQQcN",
        "outputId": "d86f2b27-e64f-4862-d352-a54481460a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CUDA available: True\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = WasteCNN(num_classes=9).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "PoKFnce6QyuG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()                 # reset previous gradients\n",
        "        outputs = model(images)               # forward pass\n",
        "        loss = criterion(outputs, labels)     # compute loss\n",
        "        loss.backward()                       # backward pass\n",
        "        optimizer.step()                      # update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print every 100 batches to check progress\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], \"\n",
        "                  f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Print average loss per epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"‚úÖ Epoch [{epoch+1}/{num_epochs}] completed ‚Äî Average Loss: {epoch_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "H4gbsIjQRzwL",
        "outputId": "e64422a7-49ca-4b6f-cde9-9bb86f5e84c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [100/104], Loss: 0.9287\n",
            "‚úÖ Epoch [1/100] completed ‚Äî Average Loss: 1.2577\n",
            "Epoch [2/100], Step [100/104], Loss: 1.1599\n",
            "‚úÖ Epoch [2/100] completed ‚Äî Average Loss: 1.2508\n",
            "Epoch [3/100], Step [100/104], Loss: 1.3003\n",
            "‚úÖ Epoch [3/100] completed ‚Äî Average Loss: 1.1670\n",
            "Epoch [4/100], Step [100/104], Loss: 1.1286\n",
            "‚úÖ Epoch [4/100] completed ‚Äî Average Loss: 1.1810\n",
            "Epoch [5/100], Step [100/104], Loss: 1.4662\n",
            "‚úÖ Epoch [5/100] completed ‚Äî Average Loss: 1.1289\n",
            "Epoch [6/100], Step [100/104], Loss: 1.2124\n",
            "‚úÖ Epoch [6/100] completed ‚Äî Average Loss: 1.1328\n",
            "Epoch [7/100], Step [100/104], Loss: 0.9224\n",
            "‚úÖ Epoch [7/100] completed ‚Äî Average Loss: 1.1003\n",
            "Epoch [8/100], Step [100/104], Loss: 1.0077\n",
            "‚úÖ Epoch [8/100] completed ‚Äî Average Loss: 1.1133\n",
            "Epoch [9/100], Step [100/104], Loss: 0.9640\n",
            "‚úÖ Epoch [9/100] completed ‚Äî Average Loss: 1.0670\n",
            "Epoch [10/100], Step [100/104], Loss: 1.4295\n",
            "‚úÖ Epoch [10/100] completed ‚Äî Average Loss: 1.0404\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}