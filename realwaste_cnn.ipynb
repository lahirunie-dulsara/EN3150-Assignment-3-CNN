{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13385463,"sourceType":"datasetVersion","datasetId":8493277}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nimport os\nimport shutil\nimport time\nimport cv2\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, ConcatDataset\nfrom PIL import Image\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:55:06.909008Z","iopub.execute_input":"2025-10-27T04:55:06.909279Z","iopub.status.idle":"2025-10-27T04:55:06.914125Z","shell.execute_reply.started":"2025-10-27T04:55:06.909260Z","shell.execute_reply":"2025-10-27T04:55:06.913407Z"},"id":"MZoPEPYVJQwY"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:50:42.567248Z","iopub.execute_input":"2025-10-27T04:50:42.567530Z","iopub.status.idle":"2025-10-27T04:50:42.633519Z","shell.execute_reply.started":"2025-10-27T04:50:42.567507Z","shell.execute_reply":"2025-10-27T04:50:42.632625Z"},"id":"JNUl-v37JQwc"},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"base_dir = \"/kaggle/input/realwaste/realwaste-main/RealWaste\"\nfor folder in os.listdir(base_dir):\n    count = len(os.listdir(os.path.join(base_dir, folder)))\n    print(f\"{folder}: {count} images\")","metadata":{"id":"eurMme0qObLX","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:50:44.752304Z","iopub.execute_input":"2025-10-27T04:50:44.752817Z","iopub.status.idle":"2025-10-27T04:50:44.849417Z","shell.execute_reply.started":"2025-10-27T04:50:44.752794Z","shell.execute_reply":"2025-10-27T04:50:44.848719Z"}},"outputs":[{"name":"stdout","text":"Metal: 790 images\nGlass: 420 images\nPaper: 500 images\nVegetation: 436 images\nCardboard: 461 images\nTextile Trash: 318 images\nFood Organics: 411 images\nPlastic: 921 images\nMiscellaneous Trash: 495 images\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Configuration\nBATCH_SIZE = 64\nIMG_SIZE = (224, 224)\nDATA_DIR = \"/kaggle/input/realwaste/realwaste-main/RealWaste\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:50:51.931358Z","iopub.execute_input":"2025-10-27T04:50:51.931921Z","iopub.status.idle":"2025-10-27T04:50:51.935347Z","shell.execute_reply.started":"2025-10-27T04:50:51.931897Z","shell.execute_reply":"2025-10-27T04:50:51.934608Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"pip install split-folders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:50:53.703265Z","iopub.execute_input":"2025-10-27T04:50:53.703975Z","iopub.status.idle":"2025-10-27T04:50:58.231718Z","shell.execute_reply.started":"2025-10-27T04:50:53.703951Z","shell.execute_reply":"2025-10-27T04:50:58.230778Z"}},"outputs":[{"name":"stdout","text":"Collecting split-folders\n  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\nDownloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import splitfolders\n\ninput_folder = base_dir\nsplit_dir = \"/kaggle/working/RealWaste_split\"\n\nsplitfolders.ratio(input_folder, output=split_dir, seed=42, ratio=(.7, .15, .15))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:50:58.233317Z","iopub.execute_input":"2025-10-27T04:50:58.233631Z","iopub.status.idle":"2025-10-27T04:51:25.319961Z","shell.execute_reply.started":"2025-10-27T04:50:58.233608Z","shell.execute_reply":"2025-10-27T04:51:25.319195Z"}},"outputs":[{"name":"stderr","text":"Copying files: 4752 files [00:27, 175.54 files/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Count images in each subfolder\nfor split in ['train', 'val', 'test']:\n    split_path = os.path.join(split_dir, split)\n    print(f\"\\n{split.upper()} SET\")\n    total = 0\n    for cls in os.listdir(split_path):\n        cls_path = os.path.join(split_path, cls)\n        count = len(os.listdir(cls_path))\n\n        total += count\n        print(f\"  {cls}: {count} images\")\n    print(f\" Total {split}: {total} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:51:35.552416Z","iopub.execute_input":"2025-10-27T04:51:35.553181Z","iopub.status.idle":"2025-10-27T04:51:35.564995Z","shell.execute_reply.started":"2025-10-27T04:51:35.553143Z","shell.execute_reply":"2025-10-27T04:51:35.564175Z"}},"outputs":[{"name":"stdout","text":"\nTRAIN SET\n  Glass: 294 images\n  Paper: 350 images\n  Plastic: 644 images\n  Textile Trash: 222 images\n  Miscellaneous Trash: 346 images\n  Metal: 553 images\n  Food Organics: 287 images\n  Cardboard: 322 images\n  Vegetation: 305 images\n Total train: 3323 images\n\nVAL SET\n  Glass: 63 images\n  Paper: 75 images\n  Plastic: 138 images\n  Textile Trash: 47 images\n  Miscellaneous Trash: 74 images\n  Metal: 118 images\n  Food Organics: 61 images\n  Cardboard: 69 images\n  Vegetation: 65 images\n Total val: 710 images\n\nTEST SET\n  Glass: 63 images\n  Paper: 75 images\n  Plastic: 139 images\n  Textile Trash: 49 images\n  Miscellaneous Trash: 75 images\n  Metal: 119 images\n  Food Organics: 63 images\n  Cardboard: 70 images\n  Vegetation: 66 images\n Total test: 719 images\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Data Augmentation\ntrain_transforms = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(degrees=45),\n        transforms.RandomResizedCrop(size=IMG_SIZE[0], scale=(0.6, 0.9)),\n        transforms.Resize(IMG_SIZE),\n        transforms.ToTensor()\n])\n\nval_test_transforms = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor(),\n])\n\ntrain_dataset = datasets.ImageFolder(\"/kaggle/working/RealWaste_split/train\", transform=train_transforms)\nval_dataset   = datasets.ImageFolder(\"/kaggle/working/RealWaste_split/val\", transform=val_test_transforms)\ntest_dataset  = datasets.ImageFolder(\"/kaggle/working/RealWaste_split/test\", transform=val_test_transforms)\n\n# Get number of classes and device\nnum_classes = len(train_dataset.classes)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:53:17.651921Z","iopub.execute_input":"2025-10-27T04:53:17.652512Z","iopub.status.idle":"2025-10-27T04:53:17.671888Z","shell.execute_reply.started":"2025-10-27T04:53:17.652492Z","shell.execute_reply":"2025-10-27T04:53:17.671075Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nNumber of classes: 9\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Increase the number of training data\nduplication_factor = 3\ntrain_dataset_augmented = ConcatDataset([train_dataset] * duplication_factor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:55:14.058584Z","iopub.execute_input":"2025-10-27T04:55:14.058887Z","iopub.status.idle":"2025-10-27T04:55:14.063071Z","shell.execute_reply.started":"2025-10-27T04:55:14.058867Z","shell.execute_reply":"2025-10-27T04:55:14.062513Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Extract targets\ntargets = []\nfor dataset in train_dataset_augmented.datasets:  # train_dataset_augmented.datasets is a list\n    if hasattr(dataset, 'targets'):\n        targets.extend(dataset.targets)\n    else:\n        # fallback if custom dataset\n        for i in range(len(dataset)):\n            _, label = dataset[i]\n            targets.append(label)\n\n# Convert to numpy\ntargets = np.array(targets)\n\n# Class counts\nclass_counts = np.bincount(targets)\nprint(\"Class counts:\", class_counts)\n\nbase_dataset = train_dataset_augmented.datasets[0]  # first dataset in the concat list\nclasses = base_dataset.classes\n\n# Compute class weights (inverse of frequency)\nclass_weights = 1. / class_counts\n\n# Ensure sample_weights are floats for the sampler\nsample_weights = [class_weights[label].item() for label in targets]\n\nprint(\"Class counts per category:\")\nfor cls, count in zip(classes, class_counts):\n    print(f\"  {cls:15s}: {count}\")\n\nprint(\"\\n Class weights (inverse of frequency):\")\nfor cls, w in zip(classes, class_weights):\n    print(f\"  {cls:15s}: {w:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:57:12.379030Z","iopub.execute_input":"2025-10-27T04:57:12.379309Z","iopub.status.idle":"2025-10-27T04:57:12.394378Z","shell.execute_reply.started":"2025-10-27T04:57:12.379291Z","shell.execute_reply":"2025-10-27T04:57:12.393713Z"}},"outputs":[{"name":"stdout","text":"Class counts: [ 966  861  882 1659 1038 1050 1932  666  915]\nClass counts per category:\n  Cardboard      : 966\n  Food Organics  : 861\n  Glass          : 882\n  Metal          : 1659\n  Miscellaneous Trash: 1038\n  Paper          : 1050\n  Plastic        : 1932\n  Textile Trash  : 666\n  Vegetation     : 915\n\n Class weights (inverse of frequency):\n  Cardboard      : 0.001035\n  Food Organics  : 0.001161\n  Glass          : 0.001134\n  Metal          : 0.000603\n  Miscellaneous Trash: 0.000963\n  Paper          : 0.000952\n  Plastic        : 0.000518\n  Textile Trash  : 0.001502\n  Vegetation     : 0.001093\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Define a consistent BATCH_SIZE\nBATCH_SIZE = 32\n\ntrain_loader = DataLoader(train_dataset_augmented, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False) \ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\nprint(f\"\\n WeightedRandomSampler created successfully!\")\nprint(f\" Total samples in epoch: {len(sample_weights)}\")\nprint(f\" Training Batch size: {train_loader.batch_size}\")\nprint(f\" Validation Batch size: {val_loader.batch_size}\")\nprint(f\" Total training batches per epoch: {len(train_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T05:19:01.288232Z","iopub.execute_input":"2025-10-27T05:19:01.288503Z","iopub.status.idle":"2025-10-27T05:19:01.293877Z","shell.execute_reply.started":"2025-10-27T05:19:01.288485Z","shell.execute_reply":"2025-10-27T05:19:01.293196Z"}},"outputs":[{"name":"stdout","text":"\n WeightedRandomSampler created successfully!\n Total samples in epoch: 9969\n Training Batch size: 32\n Validation Batch size: 32\n Total training batches per epoch: 312\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n    train_losses, val_losses = [], []\n    train_accuracies, val_accuracies = [], []\n\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0\n        correct = 0\n        total = 0\n    \n        # TRAIN LOOP WITH PROGRESS BAR\n        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\", leave=False)\n        for images, labels in train_loop:\n            images, labels = images.to(device), labels.to(device)\n    \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n    \n            running_loss += loss.item() * images.size(0)\n    \n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            train_acc = 100 * correct / total\n            train_loop.set_postfix(loss=loss.item(), acc=train_acc)\n    \n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        epoch_train_acc = 100 * correct / total\n    \n        # VALIDATION LOOP WITH PROGRESS BAR\n        model.eval()\n        val_running_loss = 0\n        val_correct = 0\n        val_total = 0\n    \n        with torch.no_grad():\n            val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\", leave=False)\n            for images, labels in val_loop:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n    \n                val_running_loss += loss.item() * images.size(0)\n    \n                _, predicted = torch.max(outputs, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n    \n        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n        epoch_val_acc = 100 * val_correct / val_total\n    \n        # Summary\n        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% | \"\n              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n\n    return train_losses, val_losses, train_accuracies, val_accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:12:12.519835Z","iopub.execute_input":"2025-10-27T07:12:12.520108Z","iopub.status.idle":"2025-10-27T07:12:12.528880Z","shell.execute_reply.started":"2025-10-27T07:12:12.520088Z","shell.execute_reply":"2025-10-27T07:12:12.528204Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"\n# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n#     train_losses, val_losses = [], []\n#     train_accuracies, val_accuracies = [], []\n\n#     for epoch in range(num_epochs):\n#         model.train()\n#         running_loss = 0\n#         correct, total = 0, 0\n\n#         train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\", leave=False)\n#         for images, labels in train_loop:\n#             images, labels = images.to(device), labels.to(device)\n\n#             optimizer.zero_grad()\n#             outputs = model(images)\n#             loss = criterion(outputs, labels)\n#             loss.backward()\n#             optimizer.step()\n\n#             running_loss += loss.item() * images.size(0)\n#             _, predicted = torch.max(outputs, 1)\n#             total += labels.size(0)\n#             correct += (predicted == labels).sum().item()\n#             train_loop.set_postfix(loss=loss.item(), acc=100*correct/total)\n\n#         epoch_train_loss = running_loss / len(train_loader.dataset)\n#         epoch_train_acc = 100 * correct / total\n\n#         model.eval()\n#         val_running_loss, val_correct, val_total = 0, 0, 0\n\n#         with torch.no_grad():\n#             val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\", leave=False)\n#             for images, labels in val_loop:\n#                 images, labels = images.to(device), labels.to(device)\n#                 outputs = model(images)\n#                 loss = criterion(outputs, labels)\n\n#                 val_running_loss += loss.item() * images.size(0)\n#                 _, predicted = torch.max(outputs, 1)\n#                 val_total += labels.size(0)\n#                 val_correct += (predicted == labels).sum().item()\n\n#         epoch_val_loss = val_running_loss / len(val_loader.dataset)\n#         epoch_val_acc = 100 * val_correct / val_total\n\n#         print(f\"Epoch {epoch+1}/{num_epochs}: \"\n#               f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% | \"\n#               f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\\n\")\n\n#         train_losses.append(epoch_train_loss)\n#         val_losses.append(epoch_val_loss)\n#         train_accuracies.append(epoch_train_acc)\n#         val_accuracies.append(epoch_val_acc)\n\n#     return train_losses, val_losses, train_accuracies, val_accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:12:12.914094Z","iopub.execute_input":"2025-10-27T07:12:12.914289Z","iopub.status.idle":"2025-10-27T07:12:12.918596Z","shell.execute_reply.started":"2025-10-27T07:12:12.914274Z","shell.execute_reply":"2025-10-27T07:12:12.917806Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self, num_classes=9):\n        super(CNNModel, self).__init__()\n\n        # Convolution Blocks\n        self.conv_block1 = self._create_conv_block(3, 64)  # 64 filtri\n        self.conv_block2 = self._create_conv_block(64, 128)  # 128 filtri\n        self.conv_block3 = self._create_conv_block(128, 256)  # 256 filtri\n        self.conv_block4 = self._create_conv_block(256, 512)  # 512 filtri\n        self.conv_block5 = self._create_conv_block(512, 512)  # 512 filtri aggiuntivi\n\n        # Global Average Pooling\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n\n        # Fully Connected Layers\n        self.fc1 = nn.Linear(512, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        self.dropout = nn.Dropout(0.3)\n\n    def _create_conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        x = self.conv_block1(x)\n        x = self.conv_block2(x)\n        x = self.conv_block3(x)\n        x = self.conv_block4(x)\n        x = self.conv_block5(x)\n\n        # Global Average Pooling\n        x = self.global_avg_pool(x)\n        x = x.view(x.size(0), -1)  # Flatten\n\n        x = nn.functional.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\nmodel = CNNModel(num_classes=9)\nmodel_name = \"Custom CNN\"\n\n# Device Configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Loss Function\nweights = torch.tensor(class_weights.copy(), dtype=torch.float32).to(device)\ncriterion = nn.CrossEntropyLoss(weight=weights)\n\nmodel = model.to(device)\n\nresults = {}\n\ndef reset_weights(model):\n    for layer in model.children():\n        if hasattr(layer, 'reset_parameters'):\n            layer.reset_parameters()\n\n\n# print(f\"\\n===== Training with Adam =====\\n\")\n\n# # Adams Optimizer \n# learning_rate = 1e-5\n# weight_decay = 1e-2\n# optimizer = optim.Adam(\n#     filter(lambda p: p.requires_grad, model.parameters()),\n#     lr=learning_rate,\n#     weight_decay=weight_decay\n# )\n\n# # Scheduler of learning rate\n# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n# num_epochs = 20\n\n# # Training the model\n# train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n#     model=model,\n#     train_loader=train_loader,\n#     val_loader=val_loader,\n#     criterion=criterion,\n#     optimizer=optimizer,\n#     num_epochs=num_epochs\n# )\n\n# # Plot Training and Validation\n# plot_training_validation_metrics(\n#     train_losses=train_losses,\n#     val_losses=val_losses,\n#     train_accuracies=train_accuracies,\n#     val_accuracies=val_accuracies\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:12:14.912225Z","iopub.execute_input":"2025-10-27T07:12:14.912817Z","iopub.status.idle":"2025-10-27T07:12:14.994494Z","shell.execute_reply.started":"2025-10-27T07:12:14.912794Z","shell.execute_reply":"2025-10-27T07:12:14.993984Z"},"id":"O-w9CQXwJQwq"},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"print(f\"\\n===== Training with Adam =====\\n\")\n\nreset_weights(model)\n\n# Adams Optimizer \nlearning_rate = 1e-5\nweight_decay = 1e-2\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=learning_rate,\n    weight_decay=weight_decay\n)\n\n# Scheduler of learning rate\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\nnum_epochs = 5\n\n# Training the model\ntrain_losses, val_losses, train_accuracies, val_accuracies = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=criterion,\n    optimizer=optimizer,\n    num_epochs=num_epochs\n)\n\n# Save the model\nresults['Adam'] = (train_losses, val_losses, train_accuracies, val_accuracies)\n\nmodel_save_path = f\"/kaggle/working/trained_{model_name}_adam.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:12:19.513465Z","iopub.execute_input":"2025-10-27T07:12:19.513745Z","iopub.status.idle":"2025-10-27T07:23:36.419924Z","shell.execute_reply.started":"2025-10-27T07:12:19.513725Z","shell.execute_reply":"2025-10-27T07:23:36.419046Z"}},"outputs":[{"name":"stdout","text":"\n===== Training with Adam =====\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: Train Loss: 1.5271, Train Acc: 47.39% | Val Loss: 1.3080, Val Acc: 56.90%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5: Train Loss: 1.1479, Train Acc: 59.47% | Val Loss: 1.1507, Val Acc: 60.00%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5: Train Loss: 0.9972, Train Acc: 64.37% | Val Loss: 0.9910, Val Acc: 65.49%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5: Train Loss: 0.8953, Train Acc: 67.79% | Val Loss: 0.9152, Val Acc: 67.61%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          ","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5: Train Loss: 0.8132, Train Acc: 70.30% | Val Loss: 0.8534, Val Acc: 71.55%\nModel saved to /kaggle/working/trained_Custom CNN_adam.pth\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"print(f\"\\n===== Training with SGD =====\\n\")\n\nreset_weights(model)\n\n# SGD Optimizer \nlearning_rate = 1e-2\nweight_decay = 1e-4\noptimizer = optim.SGD(\n    model.parameters(), \n    lr= learning_rate, \n    weight_decay= weight_decay\n)\n\n# Scheduler of learning rate\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\nnum_epochs = 5\n\n# Training the model\ntrain_losses, val_losses, train_accuracies, val_accuracies = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=criterion,\n    optimizer=optimizer,\n    num_epochs=num_epochs\n)\n\n# Save the model\nresults['SGD'] = (train_losses, val_losses, train_accuracies, val_accuracies)\n\nmodel_save_path = f\"/kaggle/working/trained_{model_name}_sgd.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T07:23:36.421139Z","iopub.execute_input":"2025-10-27T07:23:36.421347Z"}},"outputs":[{"name":"stdout","text":"\n===== Training with SGD =====\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5: Train Loss: 1.2073, Train Acc: 57.01% | Val Loss: 2.4060, Val Acc: 30.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5 (Train):   1%|‚ñè         | 4/312 [00:01<02:07,  2.41it/s, acc=61.7, loss=1.22]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(f\"\\n===== Training with SGD with Momentum =====\\n\")\n\nreset_weights(model)\n\n# SGD with Momentum Optimizer \nlearning_rate = 1e-2\nweight_decay = 1e-4\noptimizer = optim.SGD(\n    model.parameters(), \n    lr= learning_rate, \n    weight_decay= weight_decay,\n    momentum=0.9\n)\n\n# Scheduler of learning rate\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\nnum_epochs = 5\n\n# Training the model\ntrain_losses, val_losses, train_accuracies, val_accuracies = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=criterion,\n    optimizer=optimizer,\n    num_epochs=num_epochs\n)\n\n# Save the model\nresults['SGD with Momentum'] = (train_losses, val_losses, train_accuracies, val_accuracies)\n\nmodel_save_path = f\"/kaggle/working/trained_{model_name}_sgdmomentum.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nfile_path = \"/kaggle/working/initial_model_weights.pth\"  # change this to your file name\n\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(\"File deleted successfully.\")\nelse:\n    print(\"File not found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T06:31:52.788550Z","iopub.execute_input":"2025-10-27T06:31:52.789233Z","iopub.status.idle":"2025-10-27T06:31:52.801526Z","shell.execute_reply.started":"2025-10-27T06:31:52.789211Z","shell.execute_reply":"2025-10-27T06:31:52.800788Z"}},"outputs":[{"name":"stdout","text":"File deleted successfully.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import copy\nimport matplotlib.pyplot as plt\n\noptimizers = {\n    \"Adam\": optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=1e-2),\n    \"SGD\": optim.SGD(model.parameters(), lr=1e-3),\n    \"SGD+Momentum\": optim.SGD(model.parameters(), lr=1e-3, momentum=0.9),\n}\n\nresults = {}\n\nfor opt_name, optimizer in optimizers.items():\n    print(f\"\\n===== Training with {opt_name} =====\\n\")\n\n    model_temp = get_fresh_model_for_comparison(num_classes, device)\n    \n    train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n        model=model_temp,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        num_epochs=num_epochs,\n    )\n    \n    results[opt_name] = (train_losses, val_losses, train_accuracies, val_accuracies)\n\n    # Save the model\n    model_save_path = f\"/kaggle/working/trained_{model_name}_{opt_name}.pth\"\n    torch.save(model.state_dict(), model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T06:29:43.159918Z","iopub.execute_input":"2025-10-27T06:29:43.160190Z","iopub.status.idle":"2025-10-27T06:30:23.688237Z","shell.execute_reply.started":"2025-10-27T06:29:43.160171Z","shell.execute_reply":"2025-10-27T06:30:23.687253Z"}},"outputs":[{"name":"stdout","text":"\n===== Training with Adam =====\n\nCreated and saved initial model weights to /kaggle/working/initial_model_weights.pth\n","output_type":"stream"},{"name":"stderr","text":"                                                                                         \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/189676623.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fresh_model_for_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_temp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/245427375.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":38},{"cell_type":"code","source":"plt.figure(figsize=(14,6))\n\n# Loss comparison\nplt.subplot(1,2,1)\nfor name in results:\n    plt.plot(results[name][0], label=f\"{name} Train\")\n    plt.plot(results[name][1], linestyle='--', label=f\"{name} Val\")\nplt.title(\"Loss Comparison\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n# Accuracy comparison\nplt.subplot(1,2,2)\nfor name in results:\n    plt.plot(results[name][2], label=f\"{name} Train\")\n    plt.plot(results[name][3], linestyle='--', label=f\"{name} Val\")\nplt.title(\"Accuracy Comparison\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy (%)\")\nplt.legend()\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T19:15:08.777556Z","iopub.execute_input":"2025-02-03T19:15:08.777883Z","iopub.status.idle":"2025-02-03T19:15:08.875795Z","shell.execute_reply.started":"2025-02-03T19:15:08.777856Z","shell.execute_reply":"2025-02-03T19:15:08.875000Z"},"id":"gL6mDTAVJQwr"},"outputs":[],"execution_count":null}]}